\chapter{\bevezetes}

Data helps us model and understand our world more truthfully. Enabling the
processing of the ever increasing volume of data, and running more precise
simulations necessitate continuous engineering efforts.

An underlying assumption of model-reduction based techniques is that
a low-dimensional structure can be found in high-dimensional data. This same
assumption serves as the basis of many data processing techniques and
applications. An early example is the eigenface approach for solving the
computer vision problem of face recognition. \citet{eigenface1987} showed that
Principal Component Analysis (PCA) can be used on a set of face images to form
a set of basis features. These underlying principal features could then be used
to reconstruct these (or even brand new) human faces.

More formally, we can define an encoder function $f_e(\vb{x}) = \vb{c}$, that
transforms some high-dimensional discretized data $\vb{x}$ into an encoding
$\vb{c} \in \mathbb{R}^{N}$ with low dimensionality $N$. A decoder function
$f_d(\vb{c}) = \vb{x}$ reverses the mapping. Thus, $f_e$ and $f_d$ form an
encoder-decoder model. The high-dimensional data $\vb{x}$ could be anything from
images of human faces to attributes of a fluid flow, such as a spatio-temporal
pressure function or a velocity field. The goal is to find the underlying
low-dimensional structure in any of these datasets.

Whereas the evolution of physics systems is known for many practical use-cases
(see chapter~\ref{chapter:fluid-simulation} on fluid simulation), devising
a time stepping scheme in the space of the lower-dimensional embedding $\vb{c}$
is a further problem: finding a function $f_t(\vb{c}^t) = \vb{c}^{t+1}$ that
evolves our system forward in time to the next time step $\vb{c}^{t+1}$ in the
reduced dimension.

Introduced in section~\ref{section:neural-networks}, Neural Networks (NNs) are
universal function approximators.  \citet{LatentSpaceSubdivision} and
\citet{Wiewel2019LatentSP} show that it is possible to learn $f_e$, $f_d$ and
$f_t$ for fluid flow simulation via NNs.

Positioned at the crossroads of physical simulation, and deep learning
techniques, our work is inspired by current advances in physics-based deep
learning. We investigate the general problem of controlling simulation
parameters to achieve target outcomes.

Our novel method solves problems in fluid simulation by utilizing gradients of
a differentiable physics simulation. More specifically, we use gradients from
a reduced order fluid simulation technique based on eigenfunctions of the vector
Laplacian operator. Instead of simulating the full domain, a reduced dimensional
space is utilized, resulting in significant speed-ups. We show some of the
possibilities arising when differentiating a Laplacian eigenfluids simulation,
and investigate how the resulting gradients can drive optimization in different
scenarios.

In the remainder of this chapter, we briefly discuss previous research that
served as inspiration as well as a base for our current work. We also give an
overview of the overall structure of our thesis. 

\section{Previous Work}
\subsection{Fluid Simulation}
Most simulation methods are based on either an Eulerian (i.e.  grid-based), or
Lagrangian (i.e. particle-based) representation of the fluid.  For an overview
of fluid simulation techniques in computer graphics, see \citet{FluidNotes} and
\citet{BridsonFluid}. 

For advecting marker density in our fluid, as well as a comparative "baseline"
simulation, we will use Eulerian simulation techniques, mostly as described by
\citet{StableFluids}.

\subsubsection*{Reduced Order Modeling of Fluids}
Dimension reduction-based techniques have been applied to fluid simulation in
multiple previous works. \citet{Wiewel2019LatentSP} demonstrated that functions
of an evolving physics system can be predicted within the latent space of neural
networks. Their efficient encoder-decoder architecture predicted pressure
fields, yielding two orders of magnitudes faster simulation times than
a traditional pressure solver.

Recently, \citet{LatentSpaceSubdivision} predicted the
evolution of fluid flow via training a convolutional neural network (CNN) for
spatial compression, with another network predicting the temporal evolution in
this compressed subspace.  The main novelty of \cite{LatentSpaceSubdivision} was
the subdivision of the learned latent space, allowing interpretability, as well
as external control of quantities such as velocity and density of the fluid.

\subsubsection*{Eigenfluids}
Instead of \textit{learning} a reduced-order representation, another option is
to analytically derive the dimension reduction, and its time evolution.
\citet{dewitt} introduced a computationally efficient fluid simulation technique
to the computer graphics community. Rather than using an Eulerian grid or
Lagrangian particles, they represent fluid fields using a basis of global
functions defined over the entire simulation domain. The fluid velocity is
reconstructed as a linear combination of these bases.

They propose the use of Laplacian eigenfunctions as these global functions.
Following their method, the fluid simulation becomes a matter of evolving basis
coefficients in the space spanned by these eigenfunctions, resulting in
a speed-up characteristic of reduced-order methods. 

Following up on the work of \citet{dewitt}, multiple papers proposed
improvements to the use of Laplacian eigenfunctions for the simulation of
incompressible fluid flow. \citet{ModelReductionFluidSim} extended the technique
to handle arbitrarily-shaped domains. \citet{EigenfluidCompression} used Discrete
Cosine Transform (DCT) on the eigenfunctions for compression.
\citet{scalable-eigenfluids} improved scalability of the technique, and modified
the method to handle different types of boundary conditions.
\citet{scalable-eigenfluids} refer to the fluid simulation technique using
Laplacian eigenfunctions as \textit{eigenfluids}, which we also adhere to in the
following.

\subsection{Differentiable Solvers}
Differentiable solvers have shown tremendous success lately for optimization
problems, including training neural network models
\cite{holl2019pdecontrol, difftaichi, warp2022}.

\citet{holl2019pdecontrol} address grid-based solvers, noting them as
particularly appropriate for dense, volumetric phenomena. They put forth
$\Phi_{Flow}$, an open-source simulation toolkit built for optimization and
machine learning applications, written mostly in Python.

\subsubsection*{Physics-based Deep Learning}
Despite being a topic of research for a long time \cite{backprop}, the
interest in neural network algorithms is a relatively new phenomenon. This is
especially true for the use of learning-based methods in physical and numerical
simulations, which is a rapidly developing area of current research. Recently,
integrating physical solvers in such methods have been shown to outperform
previously used learning approaches \cite{solver-in-the-loop}.

Drawing on a wide breadth of current research, \citet{pbdl} give an overview of
deep learning methods in the context of physical simulations. 

\section{Structure}
In chapter~\ref{sec:mathematicalFoundations}, we give an overview of
mathematical foundations, introducing notation used throughout the text, as well
as discuss  preliminaries we base later chapters on. 

In chapter~\ref{chapter:physical-simulations}, the basics of physical
simulations are introduced. Building on the multivariable calculus introduced in
chapter~\ref{sec:mathematicalFoundations}, the concept of differentiable physics
simulation is introduced. 

Diving deeper into a more specific area of physical simulations,
chapter~\ref{chapter:fluid-simulation} gives a deep dive into fluid simulation
techniques, discussing the Laplacian eigenfluids method more in-depth in
section~\ref{section:laplacian-eigenfluids}. 

Finally, as a culmination of all that came before,
chapter~\ref{chapter:controlling-laplacian-eigenfluids} describes our proposed
method of Controlling Laplacian Eigenfluids. 

We close our work with a short discussion and possible future directions in
chapter~\ref{chapter:discussion}.
